{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a2cd0b",
   "metadata": {},
   "source": [
    "# # China Real Estate Demand Prediction — Pipeline Skeleton\n",
    "#\n",
    "# **Last generated:** 2025-10-06 07:47 UTC\n",
    "#\n",
    "# This notebook/script is a lean, safe baseline skeleton tailored to the Kaggle\n",
    "# competition *\"Real Estate Demand Prediction\"*.\n",
    "#\n",
    "# ### What you get\n",
    "# - Robust **IO setup** targeting Kaggle input paths\n",
    "# - **Custom competition metric** (two-stage MAPE-based)\n",
    "# - **Leakage-safe** rolling time-grouped cross-validation\n",
    "# - Minimal **feature factory** with lags/rollings (extend here)\n",
    "# - **Naïve** baseline (strong Stage-1 shield)\n",
    "# - **LightGBM Tweedie** model scaffold + optional XGB/CatBoost hooks\n",
    "# - **Blending + clipping** for metric safety\n",
    "# - **Submission writer** preserving `test.csv` row order\n",
    "#\n",
    "# > Notes:\n",
    "# > - Extend features in the marked sections. Keep temporal embargo ≥ max lag.\n",
    "# > - If the official epsilon/edge-case handling differs, update the metric function accordingly.\n",
    "#\n",
    "# ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973a7668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, sys, math, warnings, itertools, json\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except Exception as e:\n",
    "    lgb = None\n",
    "\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except Exception as e:\n",
    "    xgb = None\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor, Pool as CatPool\n",
    "except Exception as e:\n",
    "    CatBoostRegressor, CatPool = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "022293cd",
   "metadata": {
    "tags": [
     "Config"
    ]
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    n_folds = 5\n",
    "    embargo_months = 3             # >= max lag window used in features\n",
    "    use_target_lags = True         # if True, recursive inference to populate target lags\n",
    "    use_tweedie = True             # main LightGBM objective\n",
    "    lgb_params = dict(\n",
    "        objective = \"tweedie\",\n",
    "        tweedie_variance_power = 1.3,  # tune 1.3~1.7\n",
    "        metric = \"mae\",\n",
    "        num_leaves = 63,\n",
    "        max_depth = 8,\n",
    "        learning_rate = 0.05,\n",
    "        feature_fraction = 0.6,\n",
    "        bagging_fraction = 0.8,\n",
    "        bagging_freq = 1,\n",
    "        min_data_in_leaf = 128,\n",
    "        lambda_l1 = 0.0,\n",
    "        lambda_l2 = 5.0,\n",
    "        n_estimators = 3000,\n",
    "        verbose = -1,\n",
    "        random_state = seed\n",
    "    )\n",
    "\n",
    "    data_dir = \"/kaggle/input/china-real-estate-demand-prediction\"\n",
    "    out_dir  = \"/kaggle/working\"\n",
    "    target_col = \"amount_new_house_transactions\"  # train target column name\n",
    "    id_col     = \"id\"\n",
    "    month_col  = \"month\"\n",
    "    sector_col = \"sector\"\n",
    "    sector_int_col = \"sector_int\"\n",
    "    # files\n",
    "    files = dict(\n",
    "        new_house=\"train/new_house_transactions.csv\",\n",
    "        new_house_nb=\"train/new_house_transactions_nearby_sectors.csv\",\n",
    "        pre_owned=\"train/pre_owned_house_transactions.csv\",\n",
    "        pre_owned_nb=\"train/pre_owned_house_transactions_nearby_sectors.csv\",\n",
    "        land=\"train/land_transactions.csv\",\n",
    "        land_nb=\"train/land_transactions_nearby_sectors.csv\",\n",
    "        poi=\"train/sector_POI.csv\",\n",
    "        search=\"train/city_search_index.csv\",\n",
    "        city=\"train/city_indexes.csv\",\n",
    "        test=\"test.csv\",\n",
    "        sample=\"sample_submission.csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "569e02ce",
   "metadata": {
    "tags": [
     "Utils"
    ]
   },
   "outputs": [],
   "source": [
    "def set_seed(seed:int=42):\n",
    "    import random\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "def month_to_timestamp(m: str) -> pd.Timestamp:\n",
    "    \"\"\"\n",
    "    Parse month strings like '2019 Jan' to Timestamp at month-end.\n",
    "    \"\"\"\n",
    "    # there might be localized month abbreviations; try a few formats\n",
    "    for fmt in [\"%Y %b\", \"%Y-%m\", \"%b %Y\"]:\n",
    "        try:\n",
    "            return pd.to_datetime(m, format=fmt) + pd.offsets.MonthEnd(0)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # fallback\n",
    "    return pd.to_datetime(m) + pd.offsets.MonthEnd(0)\n",
    "\n",
    "def parse_sector(val) -> int:\n",
    "    \"\"\"\n",
    "    Convert 'sector 3' or 3 -> int 3.\n",
    "    \"\"\"\n",
    "    if pd.isna(val): return np.int16(-1)\n",
    "    if isinstance(val, (int, np.integer)): return int(val)\n",
    "    s = str(val).strip().lower()\n",
    "    for token in [\"sector\", \"_\", \"-\"]:\n",
    "        s = s.replace(token, \" \")\n",
    "    parts = s.split()\n",
    "    nums = [p for p in parts if p.isdigit()]\n",
    "    return int(nums[-1]) if nums else int(float(parts[-1])) if parts else -1\n",
    "\n",
    "def ensure_cols(df: pd.DataFrame, cols: List[str]):\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "    return df\n",
    "\n",
    "def reduce_mem(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.columns:\n",
    "        if pd.api.types.is_float_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], downcast=\"float\")\n",
    "        elif pd.api.types.is_integer_dtype(df[col]):\n",
    "            df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2002ee5",
   "metadata": {
    "tags": [
     "I/O"
    ]
   },
   "outputs": [],
   "source": [
    "def load_csv(path: str, dtype=None) -> pd.DataFrame:\n",
    "    full = os.path.join(CFG.data_dir, path)\n",
    "    if not os.path.exists(full):\n",
    "        print(f\"[WARN] Missing file: {full}\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(full, dtype=dtype)\n",
    "    return df\n",
    "\n",
    "def load_all():\n",
    "    dfs = {}\n",
    "    for k, rel in CFG.files.items():\n",
    "        dfs[k] = load_csv(rel)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8bfcf6a",
   "metadata": {
    "tags": [
     "Data Normalization"
    ]
   },
   "outputs": [],
   "source": [
    "def normalize_month_sector(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardize month -> Timestamp month-end; sector -> int.\n",
    "    Some CSVs might miss either column or encode differently; handle robustly.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if CFG.month_col in df.columns:\n",
    "        df[CFG.month_col] = df[CFG.month_col].astype(str).str.strip().apply(month_to_timestamp)\n",
    "    else:\n",
    "        # Try to split from 'id' pattern 'YYYY Mon_sector n'\n",
    "        if CFG.id_col in df.columns:\n",
    "            tmp = df[CFG.id_col].astype(str).str.split(\"_sector\", n=1, expand=True)\n",
    "            df[CFG.month_col] = tmp[0].str.strip().apply(month_to_timestamp)\n",
    "            df[CFG.sector_col] = \"sector \" + tmp[1].str.strip()\n",
    "    if CFG.sector_col in df.columns:\n",
    "        df[CFG.sector_int_col] = df[CFG.sector_col].apply(parse_sector).astype(\"int16\")\n",
    "    else:\n",
    "        # try parse from 'month' if it accidentally concatenated\n",
    "        # e.g. '2019 Jan_sector 3' stored under month\n",
    "        mask = df[CFG.month_col].astype(str).str.contains(\"sector\", case=False, na=False)\n",
    "        if mask.any():\n",
    "            ss = df.loc[mask, CFG.month_col].astype(str)\n",
    "            # split\n",
    "            mm = ss.str.split(\"_sector\", n=1, expand=True)\n",
    "            df.loc[mask, CFG.month_col] = mm[0].apply(month_to_timestamp)\n",
    "            df.loc[mask, CFG.sector_int_col] = mm[1].apply(parse_sector).astype(\"int16\")\n",
    "        else:\n",
    "            df[CFG.sector_int_col] = -1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "753f4946",
   "metadata": {
    "tags": [
     "Feature Engineering"
    ]
   },
   "outputs": [],
   "source": [
    "def add_group_lags(df: pd.DataFrame, cols: List[str], lags: List[int]) -> pd.DataFrame:\n",
    "    df = df.sort_values([CFG.sector_int_col, CFG.month_col]).copy()\n",
    "    for c in cols:\n",
    "        for lag in lags:\n",
    "            df[f\"{c}_lag{lag}\"] = df.groupby(CFG.sector_int_col, observed=True)[c].shift(lag)\n",
    "    return df\n",
    "\n",
    "def add_group_rollings(df: pd.DataFrame, cols: List[str], windows: List[int], funcs=(\"mean\",\"median\")) -> pd.DataFrame:\n",
    "    df = df.sort_values([CFG.sector_int_col, CFG.month_col]).copy()\n",
    "    for c in cols:\n",
    "        for w in windows:\n",
    "            gb = df.groupby(CFG.sector_int_col, observed=True)[c]\n",
    "            for fn in funcs:\n",
    "                df[f\"{c}_roll{w}_{fn}\"] = gb.shift(1).rolling(w, min_periods=max(1, w//2)).agg(fn).values\n",
    "    return df\n",
    "\n",
    "def build_feature_matrix(dfs: Dict[str, pd.DataFrame]) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns: train_df, test_df, features_list\n",
    "    \"\"\"\n",
    "    # Load and normalize\n",
    "    nh = normalize_month_sector(dfs.get(\"new_house\", pd.DataFrame()))\n",
    "    nh_nb = normalize_month_sector(dfs.get(\"new_house_nb\", pd.DataFrame()))\n",
    "    po = normalize_month_sector(dfs.get(\"pre_owned\", pd.DataFrame()))\n",
    "    po_nb = normalize_month_sector(dfs.get(\"pre_owned_nb\", pd.DataFrame()))\n",
    "    land = normalize_month_sector(dfs.get(\"land\", pd.DataFrame()))\n",
    "    land_nb = normalize_month_sector(dfs.get(\"land_nb\", pd.DataFrame()))\n",
    "    poi = dfs.get(\"poi\", pd.DataFrame()).copy()\n",
    "    search = dfs.get(\"search\", pd.DataFrame()).copy()\n",
    "    city = dfs.get(\"city\", pd.DataFrame()).copy()\n",
    "    test = normalize_month_sector(dfs.get(\"test\", pd.DataFrame()))\n",
    "    sample = dfs.get(\"sample\", pd.DataFrame()).copy()\n",
    "\n",
    "    # Canonical index from new_house (target CSV)\n",
    "    base = nh[[CFG.month_col, CFG.sector_int_col]].drop_duplicates().copy()\n",
    "    base[\"year\"] = base[CFG.month_col].dt.year.astype(\"int16\")\n",
    "    base[\"month_num\"] = base[CFG.month_col].dt.month.astype(\"int8\")\n",
    "\n",
    "    # Merge core numeric columns from new_house\n",
    "    use_cols_nh = [CFG.target_col,\n",
    "                   \"num_new_house_transactions\",\n",
    "                   \"area_new_house_transactions\",\n",
    "                   \"price_new_house_transactions\",\n",
    "                   \"num_new_house_available_for_sale\",\n",
    "                   \"area_new_house_available_for_sale\",\n",
    "                   \"period_new_house_sell_through\"]\n",
    "    base = base.merge(nh[[CFG.month_col, CFG.sector_int_col]+[c for c in use_cols_nh if c in nh.columns]],\n",
    "                      on=[CFG.month_col, CFG.sector_int_col], how=\"left\")\n",
    "\n",
    "    # Nearby aggregates (if present)\n",
    "    if not nh_nb.empty:\n",
    "        suf = \"_nearby\"\n",
    "        map_cols = {\n",
    "            \"num_new_house_transactions\":\"num_new_house_transactions\"+suf,\n",
    "            \"area_new_house_transactions\":\"area_new_house_transactions\"+suf,\n",
    "            \"price_new_house_transactions\":\"price_new_house_transactions\"+suf,\n",
    "            CFG.target_col:\"amount_new_house_transactions\"+suf,\n",
    "            \"num_new_house_available_for_sale\":\"num_new_house_available_for_sale\"+suf,\n",
    "            \"area_new_house_available_for_sale\":\"area_new_house_available_for_sale\"+suf,\n",
    "            \"period_new_house_sell_through\":\"period_new_house_sell_through\"+suf\n",
    "        }\n",
    "        nh_nb = nh_nb.rename(columns={k:v for k,v in map_cols.items() if k in nh_nb.columns})\n",
    "        base = base.merge(nh_nb[[CFG.month_col, CFG.sector_int_col]+[c for c in map_cols.values() if c in nh_nb.columns]],\n",
    "                          on=[CFG.month_col, CFG.sector_int_col], how=\"left\")\n",
    "\n",
    "    # Pre-owned & land (own + nearby), keep a few signals; extend as needed\n",
    "    def merge_selected(src, prefix):\n",
    "        if src.empty: return base\n",
    "        cols = [c for c in src.columns if c not in [CFG.month_col, CFG.sector_col, CFG.sector_int_col]]\n",
    "        keep = []\n",
    "        for k in cols:\n",
    "            if any(x in k for x in [\"amount\", \"price\", \"area\", \"num\", \"planned_building_area\", \"construction_area\", \"transaction_amount\"]):\n",
    "                keep.append(k)\n",
    "        sel = src[[CFG.month_col, CFG.sector_int_col]+keep].copy()\n",
    "        # rename with prefix to avoid collisions\n",
    "        sel = sel.rename(columns={c:f\"{prefix}_{c}\" for c in keep})\n",
    "        return base.merge(sel, on=[CFG.month_col, CFG.sector_int_col], how=\"left\")\n",
    "\n",
    "    base = merge_selected(po, \"po\")\n",
    "    base = merge_selected(po_nb, \"po_nb\")\n",
    "    base = merge_selected(land, \"land\")\n",
    "    base = merge_selected(land_nb, \"land_nb\")\n",
    "\n",
    "    # POI static features: compress or pass through (skeleton: pass through top-level counts)\n",
    "    if not poi.empty:\n",
    "        poi = poi.copy()\n",
    "        if CFG.sector_col in poi.columns:\n",
    "            poi[CFG.sector_int_col] = poi[CFG.sector_col].apply(parse_sector).astype(\"int16\")\n",
    "        keep_poi = [c for c in poi.columns if c not in [CFG.sector_col, CFG.sector_int_col]]\n",
    "        base = base.merge(poi[[CFG.sector_int_col]+keep_poi], on=CFG.sector_int_col, how=\"left\")\n",
    "\n",
    "    # City yearly indexes: forward fill within year\n",
    "    if not city.empty:\n",
    "        city = city.copy()\n",
    "        # align to month: join on 'year'\n",
    "        if \"city_indicator_data_year\" in city.columns:\n",
    "            city[\"year\"] = city[\"city_indicator_data_year\"].astype(int)\n",
    "        elif \"year\" not in city.columns:\n",
    "            # best-effort guess\n",
    "            for c in city.columns:\n",
    "                if c.lower().endswith(\"year\"):\n",
    "                    city[\"year\"] = city[c].astype(int); break\n",
    "        num_cols = [c for c in city.columns if c not in [\"year\",\"city_indicator_data_year\"]]\n",
    "        agg = city.groupby(\"year\", as_index=False)[num_cols].mean()\n",
    "        base = base.merge(agg, on=\"year\", how=\"left\")\n",
    "\n",
    "    # Minimal calendar encodings\n",
    "    base[\"m_sin\"] = np.sin(2*np.pi*base[\"month_num\"]/12)\n",
    "    base[\"m_cos\"] = np.cos(2*np.pi*base[\"month_num\"]/12)\n",
    "\n",
    "    # Lags/Rollings (extend this list)\n",
    "    lag_cols = [c for c in [\n",
    "        CFG.target_col,\n",
    "        \"num_new_house_transactions\",\n",
    "        \"area_new_house_transactions\",\n",
    "        \"price_new_house_transactions\",\n",
    "        \"num_new_house_transactions_nearby\",\n",
    "        \"area_new_house_transactions_nearby\",\n",
    "        \"price_new_house_transactions_nearby\",\n",
    "        \"land_transaction_amount\",\n",
    "        \"land_planned_building_area\",\n",
    "    ] if c in base.columns]\n",
    "\n",
    "    base = add_group_lags(base, lag_cols, lags=[1,2,3,6,12])\n",
    "    base = add_group_rollings(base, lag_cols, windows=[3,6,12])\n",
    "\n",
    "    # Sort & memory\n",
    "    base = base.sort_values([CFG.month_col, CFG.sector_int_col]).reset_index(drop=True)\n",
    "    base = reduce_mem(base)\n",
    "\n",
    "    # Split into train rows with target and test rows from test.csv\n",
    "    test = test.copy()\n",
    "    if CFG.id_col in test.columns and CFG.month_col not in test.columns:\n",
    "        # need to split id => month + sector\n",
    "        tmp = test[CFG.id_col].astype(str).str.split(\"_sector\", n=1, expand=True)\n",
    "        test[CFG.month_col] = tmp[0].str.strip().apply(month_to_timestamp)\n",
    "        test[CFG.sector_int_col] = tmp[1].apply(parse_sector).astype(\"int16\")\n",
    "    test = test[[CFG.id_col, CFG.month_col, CFG.sector_int_col]].copy()\n",
    "\n",
    "    train_df = base[~base[CFG.target_col].isna()].copy()\n",
    "    test_df  = base.merge(test, on=[CFG.month_col, CFG.sector_int_col], how=\"right\", suffixes=(\"\",\"\"))\n",
    "\n",
    "    # Features list\n",
    "    feats = [c for c in train_df.columns if c not in\n",
    "             [CFG.target_col, CFG.id_col, CFG.sector_col, CFG.sector_int_col, CFG.month_col, \"year\", \"month_num\"]]\n",
    "\n",
    "    # Zero hints: missing month-sector in train means true 0 => keep as is; lags handle naturally\n",
    "    return train_df, test_df, feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b511f0",
   "metadata": {
    "tags": [
     "Custom Competition Metric"
    ]
   },
   "outputs": [],
   "source": [
    "def comp_score(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-6) -> float:\n",
    "    \"\"\"\n",
    "    Two-stage custom score described in the Data tab.\n",
    "    Stage 1: if >30% rows have APE > 1 (100%), score = 0\n",
    "    Stage 2: compute MAPE on rows with APE <= 1; divide by fraction kept; score = 1 - scaled MAPE.\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = np.maximum(np.abs(y_true), eps)\n",
    "    ape = np.abs(y_true - y_pred) / denom\n",
    "\n",
    "    fail_rate = (ape > 1.0).mean()\n",
    "    if fail_rate > 0.30:\n",
    "        return 0.0\n",
    "\n",
    "    mask = ape <= 1.0\n",
    "    if mask.sum() == 0:\n",
    "        return 0.0\n",
    "    mape = ape[mask].mean()\n",
    "    frac = mask.mean()\n",
    "    scaled = mape / frac\n",
    "    return float(1.0 - scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed0eab94",
   "metadata": {
    "tags": [
     "CV Splitter"
    ]
   },
   "outputs": [],
   "source": [
    "def rolling_time_splits(df: pd.DataFrame, n_folds: int, embargo_months: int) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Returns list of (train_idx, valid_idx) using month as time key.\n",
    "    Embargo period removes last `embargo_months` from train before validation start.\n",
    "    \"\"\"\n",
    "    uniq_months = np.array(sorted(df[CFG.month_col].unique()))\n",
    "    folds = []\n",
    "    # leave at least last fold for validation; simple uniform split by month\n",
    "    month_splits = np.array_split(uniq_months, n_folds + 1)  # n_folds valid blocks; first chunk(s) train start\n",
    "    # build folds by progressively expanding train, next chunk as valid\n",
    "    train_months = np.concatenate(month_splits[:-1])\n",
    "    start = 0\n",
    "    for i in range(n_folds):\n",
    "        valid_months = month_splits[i+1]\n",
    "        # embargo: drop last X train months that are too close to validation\n",
    "        if embargo_months > 0:\n",
    "            last_train_cut = valid_months.min() - pd.offsets.MonthEnd(embargo_months)\n",
    "            tr_mask = (df[CFG.month_col] <= last_train_cut)\n",
    "        else:\n",
    "            tr_mask = df[CFG.month_col].isin(np.concatenate(month_splits[:i+1]))\n",
    "        va_mask = df[CFG.month_col].isin(valid_months)\n",
    "        tr_idx = df.index[tr_mask].to_numpy()\n",
    "        va_idx = df.index[va_mask].to_numpy()\n",
    "        if len(tr_idx) == 0 or len(va_idx) == 0:\n",
    "            continue\n",
    "        folds.append((tr_idx, va_idx))\n",
    "    return folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b894d60",
   "metadata": {
    "tags": [
     "Model"
    ]
   },
   "outputs": [],
   "source": [
    "def fit_lgbm(X_tr, y_tr, X_va, y_va, params):\n",
    "    if lgb is None:\n",
    "        print(\"[WARN] lightgbm not available; skipping.\")\n",
    "        return None\n",
    "    train_set = lgb.Dataset(X_tr, label=y_tr)\n",
    "    valid_set = lgb.Dataset(X_va, label=y_va)\n",
    "    model = lgb.train(\n",
    "        params=params,\n",
    "        train_set=train_set,\n",
    "        valid_sets=[train_set, valid_set],\n",
    "        valid_names=[\"train\",\"valid\"],\n",
    "        num_boost_round=params.get(\"n_estimators\", 3000),\n",
    "        early_stopping_rounds=200,\n",
    "        verbose_eval=200\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(train_df: pd.DataFrame, feats: List[str]) -> Dict:\n",
    "    oof = np.zeros(len(train_df), dtype=float)\n",
    "    models = []\n",
    "    folds = rolling_time_splits(train_df, CFG.n_folds, CFG.embargo_months)\n",
    "    print(f\"[CV] Folds: {len(folds)}\")\n",
    "\n",
    "    for i, (tr_idx, va_idx) in enumerate(folds, 1):\n",
    "        tr = train_df.iloc[tr_idx]; va = train_df.iloc[va_idx]\n",
    "        X_tr, y_tr = tr[feats], tr[CFG.target_col]\n",
    "        X_va, y_va = va[feats], va[CFG.target_col]\n",
    "\n",
    "        params = CFG.lgb_params.copy()\n",
    "        if not CFG.use_tweedie:\n",
    "            params[\"objective\"] = \"mae\"  # alternative robust\n",
    "            params.pop(\"tweedie_variance_power\", None)\n",
    "\n",
    "        model = fit_lgbm(X_tr, y_tr, X_va, y_va, params=params)\n",
    "        if model is not None:\n",
    "            pred = model.predict(X_va, num_iteration=model.best_iteration)\n",
    "        else:\n",
    "            # fallback naive if LGBM not available\n",
    "            pred = tr.groupby(CFG.sector_int_col)[CFG.target_col].transform(\"last\").reindex(va.index).fillna(y_tr.median()).values\n",
    "\n",
    "        oof[va_idx] = np.clip(pred, 0, None)\n",
    "        models.append(model)\n",
    "        score = comp_score(y_va.values, oof[va_idx])\n",
    "        print(f\"[Fold {i}] valid score: {score:.6f}  (n_va={len(va)})\")\n",
    "\n",
    "    overall = comp_score(train_df[CFG.target_col].values, oof)\n",
    "    print(f\"[OOF] Overall score: {overall:.6f}\")\n",
    "    return {\"oof\": oof, \"models\": models, \"folds\": folds, \"feats\": feats, \"oof_score\": overall}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f1d7a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_by_sector(train_df: pd.DataFrame, test_df: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict last known target per sector.\n",
    "    \"\"\"\n",
    "    last = (train_df\n",
    "            .sort_values([CFG.sector_int_col, CFG.month_col])\n",
    "            .groupby(CFG.sector_int_col)[CFG.target_col]\n",
    "            .last())\n",
    "    # map to test\n",
    "    m = test_df[CFG.sector_int_col].map(last).fillna(train_df[CFG.target_col].median())\n",
    "    return m.to_numpy()\n",
    "\n",
    "def blend_safe(pred_main: np.ndarray, pred_naive: np.ndarray, w_main: float = 0.6) -> np.ndarray:\n",
    "    w_main = float(np.clip(w_main, 0.0, 1.0))\n",
    "    return np.clip(w_main * pred_main + (1.0 - w_main) * pred_naive, 0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c693c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(models: List, train_df: pd.DataFrame, test_df: pd.DataFrame, feats: List[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict test rows. For skeleton we use average of LGBMs; if no model, fall back to naive.\n",
    "    \"\"\"\n",
    "    if not models or models[0] is None:\n",
    "        print(\"[INFO] Using naïve predictions (no models available).\")\n",
    "        return naive_by_sector(train_df, test_df)\n",
    "\n",
    "    preds = []\n",
    "    for m in models:\n",
    "        if m is None: continue\n",
    "        preds.append(m.predict(test_df[feats], num_iteration=m.best_iteration))\n",
    "    if not preds:\n",
    "        return naive_by_sector(train_df, test_df)\n",
    "    p = np.mean(preds, axis=0)\n",
    "    return np.clip(p, 0, None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
